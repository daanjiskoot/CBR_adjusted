{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 2: Machine Learning for Drug Discovery\n\nIn this notebook, you will use **machine learning** to predict whether molecules can block the Estrogen Receptor alpha (ERα).\n\n## What is Machine Learning?\n\nMachine learning (ML) is when a computer learns patterns from data, without being explicitly programmed. Think of it like this:\n\n- **Training**: You show the computer thousands of molecules and tell it which ones are \"active\" (they block the target) and which ones are \"inactive\" (they don't).\n- **Predicting**: After seeing enough examples, the computer learns the patterns and can predict whether a *brand new* molecule will be active or not — even one that has never been tested in a lab!\n\nIt's similar to how you learn to recognise cats vs. dogs from photos — after seeing enough examples, you can tell them apart even in photos you've never seen before.\n\n## Why is this useful for drug discovery?\n\nTesting molecules in the lab is slow and expensive. With ML, we can:\n- Screen **millions** of molecules on a computer in minutes\n- Focus lab experiments on the most promising candidates\n- Save years of work and millions of euros\n\n## What will we do in this notebook?\n\n1. **Download** real experimental data on ERα from the ChEMBL database\n2. **Encode** molecules into numbers (\"fingerprints\") that the computer can understand\n3. **Train** 3 different ML models: Random Forest, SVM, and Neural Network\n4. **Evaluate** how well the models perform (are they actually learning something useful?)\n5. **Predict** the activity of new molecules you choose!\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install rdkit scikit-learn chembl_webresource_client tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from warnings import filterwarnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm, metrics, clone\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, matthews_corrcoef,\n",
    "    roc_curve, roc_auc_score, mean_absolute_error, mean_squared_error\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MACCSkeys, AllChem\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "filterwarnings('ignore')\n",
    "SEED = 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Download ERα data from ChEMBL\n\n[ChEMBL](https://www.ebi.ac.uk/chembl/) is a huge public database maintained by the European Bioinformatics Institute (EBI). It contains millions of experimental measurements: researchers around the world test molecules in the lab and upload their results to ChEMBL.\n\nWe'll download all the binding data for ERα automatically using Python — no manual downloading needed!\n\n> **What's happening here?** We're asking ChEMBL: *\"Give me all molecules that have been tested for binding to ERα (target CHEMBL206), where the measurement type is IC50 and it's a direct binding assay.\"*\n>\n> **IC50** = the concentration of drug needed to block 50% of the protein. Lower IC50 = stronger drug.\n>\n> **pChEMBL** = a standardized version of IC50 on a log scale. Higher pChEMBL = stronger binding. Think of it like a \"binding score\" where bigger is better."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chembl_webresource_client.new_client import new_client\n",
    "\n",
    "CHEMBL_TARGET = 'CHEMBL206'  # ChEMBL ID for Estrogen Receptor alpha\n",
    "\n",
    "# Fetch bioactivity data\n",
    "activity = new_client.activity\n",
    "results = activity.filter(\n",
    "    target_chembl_id=CHEMBL_TARGET,\n",
    "    type='IC50',                      # IC50 is a common measure of drug potency\n",
    "    relation='=',                     # Only exact measurements\n",
    "    assay_type='B'                    # Binding assays only\n",
    ").only([\n",
    "    'molecule_chembl_id', 'canonical_smiles', 'pchembl_value', 'assay_type',\n",
    "    'standard_relation', 'standard_value'\n",
    "])\n",
    "\n",
    "# Convert to a pandas DataFrame\n",
    "data = pd.DataFrame(results)\n",
    "print(f\"Downloaded {len(data)} datapoints from ChEMBL for {CHEMBL_TARGET}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Clean the data\n\nReal-world data is always messy! Some entries might have missing values, duplicates, or inconsistent measurements. Before we can train a model, we need to tidy things up:\n\n- Keep only the columns we need (molecule ID, SMILES, pChEMBL value)\n- Remove entries with missing values\n- Remove duplicate molecules\n- Label each molecule as **active** or **inactive**\n\n> **What's happening here?**\n>\n> We set an activity threshold: **pChEMBL ≥ 6.5 → active**. This roughly corresponds to a molecule that binds with a strength of at least ~300 nM (nanomolar), which is the ballpark for a useful drug candidate.\n>\n> **SMILES** is a way to write chemical structures as text. For example, `CCO` is ethanol (the alcohol in drinks). Every molecule in ChEMBL has a SMILES string that describes its structure."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for convenience\n",
    "pd_data = data[['molecule_chembl_id', 'pchembl_value', 'canonical_smiles']].copy()\n",
    "pd_data.columns = ['Molecule_ChEMBL_ID', 'pChEMBL_value', 'Smiles']\n",
    "\n",
    "# Convert pChEMBL to numeric (some entries may be text)\n",
    "pd_data['pChEMBL_value'] = pd.to_numeric(pd_data['pChEMBL_value'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing values\n",
    "pd_data.dropna(subset=['pChEMBL_value', 'Smiles'], inplace=True)\n",
    "\n",
    "# Remove duplicates (keep the first measurement for each molecule)\n",
    "pd_data.drop_duplicates(subset='Smiles', keep='first', inplace=True)\n",
    "pd_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Add activity label: active if pChEMBL >= 6.5\n",
    "pd_data['active'] = (pd_data['pChEMBL_value'] >= 6.5).astype(float)\n",
    "\n",
    "print(f\"After cleaning: {len(pd_data)} molecules\")\n",
    "print(f\"  Active molecules:   {int(pd_data.active.sum())}\")\n",
    "print(f\"  Inactive molecules: {len(pd_data) - int(pd_data.active.sum())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Convert molecules to fingerprints\n\nComputers can't look at a chemical drawing and understand it. We need to convert each molecule into a list of numbers — a **molecular fingerprint**.\n\n> **What's happening here?**\n>\n> Think of a fingerprint as a **barcode** for a molecule. It's a long list of 0s and 1s, where each position answers a yes/no question about the molecule's structure:\n> - Position 42: \"Does this molecule contain a nitrogen-hydrogen bond?\" → 1 (yes) or 0 (no)\n> - Position 99: \"Does this molecule have a six-membered ring?\" → 1 or 0\n> - ...and so on for 167 questions (MACCS keys) or 2048 questions (Morgan fingerprints)\n>\n> **Why does this work?** Molecules with similar structures tend to have similar fingerprints, and molecules with similar structures tend to have similar biological activity. So the ML model can learn: *\"fingerprints that look like THIS tend to be active.\"*\n>\n> We start with **MACCS keys** (167 bits, simple) and later switch to **Morgan fingerprints** (2048 bits, more detailed) for better accuracy."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_fp(smiles, method='maccs', n_bits=2048):\n",
    "    \"\"\"Convert a SMILES string to a molecular fingerprint.\"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "    if method == 'maccs':\n",
    "        return list(MACCSkeys.GenMACCSKeys(mol))\n",
    "    elif method == 'morgan2':\n",
    "        fp_gen = AllChem.GetMorganGenerator(radius=2, fpSize=n_bits)\n",
    "        return list(fp_gen.GetFingerprint(mol))\n",
    "    elif method == 'morgan3':\n",
    "        fp_gen = AllChem.GetMorganGenerator(radius=3, fpSize=n_bits)\n",
    "        return list(fp_gen.GetFingerprint(mol))\n",
    "    else:\n",
    "        return list(MACCSkeys.GenMACCSKeys(mol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fingerprints for all molecules\n",
    "tqdm.pandas()\n",
    "compound_df = pd_data.copy()\n",
    "compound_df['fp'] = compound_df['Smiles'].progress_apply(smiles_to_fp)\n",
    "compound_df.dropna(subset='fp', inplace=True)\n",
    "compound_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Generated fingerprints for {len(compound_df)} molecules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Split the data into training and test sets\n",
    "\n",
    "We split our data into two parts:\n",
    "- **Training set** (80%): used to teach the model\n",
    "- **Test set** (20%): used to check if the model actually learned something useful\n",
    "\n",
    "> **What's happening here?** It's like studying for an exam with practice problems (training), and then taking the real exam (test) with questions you haven't seen before. If you score well on the exam, you actually understood the material!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprints = np.array(compound_df.fp.tolist())\n",
    "labels = compound_df.active.tolist()\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "    fingerprints, labels, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(train_x)} molecules\")\n",
    "print(f\"Test set:     {len(test_x)} molecules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Train Machine Learning models\n\nNow the fun part! We'll try three completely different ML algorithms and see which one works best. Each algorithm learns in its own way:\n\n1. **Random Forest (RF)**: Imagine 100 people each making a flowchart of yes/no questions about a molecule (e.g., \"Does it have a ring? → yes → Does it have nitrogen? → yes → probably active\"). Each person makes a slightly different flowchart. The final prediction is whatever the majority votes for. That's a Random Forest — an \"ensemble\" of decision trees.\n\n2. **Support Vector Machine (SVM)**: Imagine plotting all molecules in a high-dimensional space (one axis per fingerprint bit). SVM finds the best \"dividing surface\" that separates active molecules on one side from inactive ones on the other.\n\n3. **Neural Network (ANN)**: Loosely inspired by the brain. Data flows through layers of interconnected \"neurons\", where each neuron applies a mathematical function. The network adjusts its connections during training until it gets good at the task.\n\n### Helper functions\n\nThese functions help us measure how good each model is and plot the results."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance(ml_model, test_x, test_y, verbose=True):\n",
    "    \"\"\"Calculate and print how well a model performs.\"\"\"\n",
    "    test_prob = ml_model.predict_proba(test_x)[:, 1]\n",
    "    test_pred = ml_model.predict(test_x)\n",
    "\n",
    "    accuracy = accuracy_score(test_y, test_pred)\n",
    "    sens = recall_score(test_y, test_pred)\n",
    "    spec = recall_score(test_y, test_pred, pos_label=0)\n",
    "    auc = roc_auc_score(test_y, test_prob)\n",
    "    bal_accuracy = (sens + spec) / 2\n",
    "    mcc = matthews_corrcoef(test_y, test_pred)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"  Accuracy:     {accuracy:.2f}\")\n",
    "        print(f\"  Sensitivity:  {sens:.2f}  (how well it finds active molecules)\")\n",
    "        print(f\"  Specificity:  {spec:.2f}  (how well it rejects inactive molecules)\")\n",
    "        print(f\"  AUC:          {auc:.2f}  (overall performance, 1.0 = perfect)\")\n",
    "\n",
    "    return accuracy, sens, spec, auc, bal_accuracy, mcc\n",
    "\n",
    "\n",
    "def plot_roc_curves(models, test_x, test_y):\n",
    "    \"\"\"Plot ROC curves for all trained models.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "    for model_info in models:\n",
    "        ml_model = model_info['model']\n",
    "        test_prob = ml_model.predict_proba(test_x)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(test_y, test_prob)\n",
    "        auc_score = roc_auc_score(test_y, test_prob)\n",
    "        ax.plot(fpr, tpr, label=f\"{model_info['label']} (AUC = {auc_score:.2f})\")\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], 'r--', label='Random guessing')\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('ROC Curves: How good is each model?')\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Model 1: Random Forest\n\nThe Random Forest builds 100 decision trees, each trained on a slightly different random subset of the data. This makes it robust — even if one tree makes a mistake, the majority vote is usually correct."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest\n",
    "model_RF = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=SEED)\n",
    "model_RF.fit(train_x, train_y)\n",
    "\n",
    "print(\"Random Forest performance:\")\n",
    "_ = model_performance(model_RF, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Model 2: Support Vector Machine (SVM)\n\nSVM uses a mathematical trick called the \"kernel trick\" (here: radial basis function / RBF) to find complex boundaries between active and inactive molecules, even when the data isn't neatly separable by a straight line."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an SVM\n",
    "model_SVM = svm.SVC(kernel='rbf', C=1, gamma=0.1, probability=True, random_state=SEED)\n",
    "model_SVM.fit(train_x, train_y)\n",
    "\n",
    "print(\"SVM performance:\")\n",
    "_ = model_performance(model_SVM, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Model 3: Neural Network (ANN)\n\nOur neural network has 2 hidden layers (with 5 and 3 neurons). It's small by modern AI standards, but often enough for molecular data. Neural networks shine when you have very large datasets — with smaller data, simpler models like RF and SVM sometimes do equally well or better."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Neural Network\n",
    "model_ANN = MLPClassifier(hidden_layer_sizes=(5, 3), random_state=SEED, max_iter=500)\n",
    "model_ANN.fit(train_x, train_y)\n",
    "\n",
    "print(\"Neural Network performance:\")\n",
    "_ = model_performance(model_ANN, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Compare all three models with ROC curves\n\nNow let's compare all three models visually using **ROC curves**.\n\n> **What's happening here?**\n>\n> The ROC (Receiver Operating Characteristic) curve shows how well a model separates active from inactive molecules. It plots two things:\n> - **True Positive Rate** (y-axis): Of all truly active molecules, how many did we correctly identify? (higher = better)\n> - **False Positive Rate** (x-axis): Of all truly inactive molecules, how many did we accidentally flag as active? (lower = better)\n>\n> A perfect model hugs the top-left corner. A random coin flip gives the diagonal red dashed line.\n>\n> The **AUC** (Area Under the Curve) summarizes this in one number:\n> - **AUC = 1.0** → perfect predictions\n> - **AUC = 0.5** → no better than random guessing\n> - **AUC > 0.8** → generally considered a good model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    {'label': 'Random Forest', 'model': model_RF},\n",
    "    {'label': 'SVM', 'model': model_SVM},\n",
    "    {'label': 'Neural Network', 'model': model_ANN},\n",
    "]\n",
    "\n",
    "plot_roc_curves(models, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Cross-validation\n\nThe results above used a single random train/test split. But what if we got lucky (or unlucky) with that particular split? **Cross-validation** repeats the experiment multiple times to give us a more reliable answer.\n\n> **What's happening here?**\n>\n> We split all the data into 3 equal parts (called \"folds\"):\n> - **Round 1**: Train on folds 1+2, test on fold 3\n> - **Round 2**: Train on folds 1+3, test on fold 2\n> - **Round 3**: Train on folds 2+3, test on fold 1\n>\n> Every molecule gets to be in the test set exactly once. We then report the average performance across all rounds. If the numbers are consistent (low standard deviation), the model is reliable.\n>\n> This is a standard practice in machine learning — you should always cross-validate before trusting your model!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidation(ml_model, df, n_folds=3):\n",
    "    \"\"\"Run cross-validation and print average performance.\"\"\"\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "    auc_per_fold = []\n",
    "    acc_per_fold = []\n",
    "\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        fold_model = clone(ml_model)\n",
    "\n",
    "        fold_train_x = df.iloc[train_index].fp.tolist()\n",
    "        fold_train_y = df.iloc[train_index].active.tolist()\n",
    "        fold_test_x = df.iloc[test_index].fp.tolist()\n",
    "        fold_test_y = df.iloc[test_index].active.tolist()\n",
    "\n",
    "        fold_model.fit(fold_train_x, fold_train_y)\n",
    "        accuracy, sens, spec, auc, bal_acc, mcc = model_performance(\n",
    "            fold_model, fold_test_x, fold_test_y, verbose=False\n",
    "        )\n",
    "        auc_per_fold.append(auc)\n",
    "        acc_per_fold.append(accuracy)\n",
    "\n",
    "    print(f\"  Mean AUC:      {np.mean(auc_per_fold):.2f} +/- {np.std(auc_per_fold):.2f}\")\n",
    "    print(f\"  Mean Accuracy: {np.mean(acc_per_fold):.2f} +/- {np.std(acc_per_fold):.2f}\")\n",
    "\n",
    "\n",
    "N_FOLDS = 3\n",
    "\n",
    "for model_info in models:\n",
    "    print(f\"\\n--- {model_info['label']} ---\")\n",
    "    crossvalidation(model_info['model'], compound_df, n_folds=N_FOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 7: Regression — predict *how strongly* a molecule binds\n\nSo far, we've been doing **classification**: just predicting \"active\" or \"inactive\" (a yes/no answer). Now let's try **regression** — predicting the actual pChEMBL value as a continuous number.\n\n> **What's happening here?**\n>\n> - **Classification** = \"Is this molecule active?\" → Yes or No\n> - **Regression** = \"What is this molecule's pChEMBL value?\" → e.g., 7.3\n>\n> This is like the difference between pass/fail grading and giving a percentage score. Regression gives us more information — we can rank molecules from strongest to weakest binder.\n>\n> We also switch to **Morgan fingerprints** (2048 bits) instead of MACCS keys (167 bits). More bits = more detailed description of the molecule = the model has more information to work with.\n>\n> **How do we measure regression performance?**\n> - **MAE** (Mean Absolute Error): On average, how far off are our predictions? (lower = better)\n> - **RMSE** (Root Mean Squared Error): Similar, but penalizes big errors more heavily. (lower = better)\n> - Rule of thumb: MAE < 0.6 and RMSE < 1.0 is considered decent for pChEMBL prediction."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to Morgan3 fingerprints (longer, more detailed barcodes)\n",
    "compound_df_reg = compound_df.copy()\n",
    "tqdm.pandas()\n",
    "compound_df_reg['fp'] = compound_df_reg['Smiles'].progress_apply(\n",
    "    smiles_to_fp, args=('morgan3',)\n",
    ")\n",
    "compound_df_reg.dropna(subset=['fp', 'pChEMBL_value'], inplace=True)\n",
    "compound_df_reg.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Molecules for regression: {len(compound_df_reg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for regression\n",
    "regressor = RandomForestRegressor(random_state=SEED)\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "mae_folds = []\n",
    "rmse_folds = []\n",
    "trained_model = None\n",
    "\n",
    "for train_idx, test_idx in kf.split(compound_df_reg):\n",
    "    fold_model = clone(regressor)\n",
    "    fold_train_x = compound_df_reg.iloc[train_idx].fp.tolist()\n",
    "    fold_train_y = compound_df_reg.iloc[train_idx].pChEMBL_value.tolist()\n",
    "    fold_test_x = compound_df_reg.iloc[test_idx].fp.tolist()\n",
    "    fold_test_y = compound_df_reg.iloc[test_idx].pChEMBL_value.tolist()\n",
    "\n",
    "    fold_model.fit(fold_train_x, fold_train_y)\n",
    "    preds = fold_model.predict(fold_test_x)\n",
    "\n",
    "    mae_folds.append(mean_absolute_error(fold_test_y, preds))\n",
    "    rmse_folds.append(np.sqrt(mean_squared_error(fold_test_y, preds)))\n",
    "    trained_model = fold_model  # Keep the last trained model for predictions\n",
    "\n",
    "print(f\"Regression results (Random Forest):\")\n",
    "print(f\"  MAE:  {np.mean(mae_folds):.2f} +/- {np.std(mae_folds):.2f}\")\n",
    "print(f\"  RMSE: {np.mean(rmse_folds):.2f} +/- {np.std(rmse_folds):.2f}\")\n",
    "print(f\"\\nMAE < 0.6 and RMSE < 1.0 is considered good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 8: Predict activity for new molecules!\n\nNow let's use our trained regression model to predict how strongly some well-known ER drugs bind. These are real drugs that have been used in medicine:\n\n- **4-hydroxytamoxifen** — the active metabolite of tamoxifen, our reference drug from Notebook 1\n- **Estradiol** — the natural estrogen hormone that normally activates ERα\n- **Tamoxifen** — one of the most widely used breast cancer drugs (a \"prodrug\" that gets converted to 4-hydroxytamoxifen in the body)\n- **Raloxifene** — another ERα blocker, used for both breast cancer prevention and osteoporosis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMILES for our test molecules\n",
    "test_molecules = {\n",
    "    '4-Hydroxytamoxifen': 'OC1=CC=C(/C(=C(/CC)C2=CC=CC=C2)C3=CC=C(OCCN(C)C)C=C3)C=C1',\n",
    "    'Estradiol':          'O[C@@H]1CC[C@@H]2[C@H]3CCC4=CC(=CC=C4[C@@H]3CC[C@]12C)O',\n",
    "    'Tamoxifen':          'CCC(/C1=CC=CC=C1)=C(\\C2=CC=C(OCCN(C)C)C=C2)C3=CC=CC=C3',\n",
    "    'Raloxifene':         'OC1=CC=C(C(=O)C2=CC=C(OCCN3CCCCC3)C=C2)C=C1/C(=C/4\\SC5=CC=C(O)C=C5)C4=O',\n",
    "}\n",
    "\n",
    "print(\"Predicted pChEMBL values for ER\\u03b1:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "predictions_for_nb3 = {}\n",
    "for name, smiles in test_molecules.items():\n",
    "    fp = smiles_to_fp(smiles, 'morgan3')\n",
    "    if fp is not np.nan:\n",
    "        pred = trained_model.predict([fp])[0]\n",
    "        predictions_for_nb3[name] = pred\n",
    "        status = 'ACTIVE' if pred >= 6.5 else 'inactive'\n",
    "        print(f\"  {name:25s}  pChEMBL = {pred:.2f}  ({status})\")\n",
    "    else:\n",
    "        print(f\"  {name:25s}  Could not parse SMILES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**How do these predictions compare to reality?** You can look up the experimental pChEMBL values for these molecules on [ChEMBL](https://www.ebi.ac.uk/chembl/) to check!\n\nIn Notebook 3, we'll use **molecular docking** to predict binding from a completely different angle — fitting 3D molecular shapes into the protein's binding pocket. Then we can compare the ML predictions above with the docking results and see if both methods agree.\n\n---\n\n## Summary\n\nIn this notebook, you learned how to:\n\n1. **Download** experimental data from ChEMBL using Python\n2. **Clean** messy real-world data and label molecules as active/inactive\n3. **Encode** molecules as fingerprints — numerical \"barcodes\" that ML models can understand\n4. **Train** three classification models (Random Forest, SVM, Neural Network) and compare them with ROC curves\n5. **Cross-validate** to make sure the models are reliable\n6. **Train** a regression model to predict continuous binding strength (pChEMBL values)\n7. **Predict** the activity of new, untested molecules\n\n**Key takeaway**: With just a few lines of code, we built models that can predict whether a molecule binds to ERα — without doing a single lab experiment!\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Try it yourself!\n\nWant to test your own molecules? You can use the **PubChem Sketcher** to draw molecules or search them by name:\n\n1. Go to the [PubChem Sketcher](https://pubchem.ncbi.nlm.nih.gov/edit3/index.html)\n2. Either **draw** a molecule using the tools, or click the **search icon** and type a molecule name (e.g., \"ibuprofen\", \"caffeine\", \"aspirin\")\n3. Once your molecule appears, look for the **SMILES** string (it appears below the drawing)\n4. Copy the SMILES and paste it in the cell below\n5. Run the cell to get a prediction!\n\nYou can also search for molecules directly on [PubChem](https://pubchem.ncbi.nlm.nih.gov/) — search by name, then find the \"Canonical SMILES\" under section 2.1.4."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TRY IT YOURSELF ---\n",
    "# Paste SMILES strings below to predict their activity\n",
    "\n",
    "my_molecules = [\n",
    "    # 'paste_your_SMILES_here',\n",
    "    # 'another_SMILES_here',\n",
    "]\n",
    "\n",
    "if my_molecules:\n",
    "    print(\"Your predictions:\")\n",
    "    for smiles in my_molecules:\n",
    "        fp = smiles_to_fp(smiles, 'morgan3')\n",
    "        if fp is not np.nan:\n",
    "            pred = trained_model.predict([fp])[0]\n",
    "            status = 'ACTIVE' if pred >= 6.5 else 'inactive'\n",
    "            print(f\"  {smiles[:50]:50s}  pChEMBL = {pred:.2f}  ({status})\")\n",
    "        else:\n",
    "            print(f\"  Could not parse: {smiles[:50]}\")\n",
    "else:\n",
    "    print(\"Add your SMILES strings to the list above and re-run this cell!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}